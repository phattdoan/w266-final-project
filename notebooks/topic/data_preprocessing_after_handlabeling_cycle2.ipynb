{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate three dfs\n",
    "\n",
    "- df_handlabled    --- handlabeled xlsx in data_cycle2 on s3 (uploaed from google drive directly)\n",
    "- df_original --- merge( train and test in data_cycle1 on s3)\n",
    "- df_temp --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Install a pip package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install s3fs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "URL0 = 's3://smart-newsdev-dmp/tmp/data/classification/data_handlabeling_cycle2/error_handlabeled_combined.xlsx'\n",
    "URL1 = 's3://smart-newsdev-dmp/tmp/data/classification/data_handlabeling_cycle1/train.csv'\n",
    "URL2 = 's3://smart-newsdev-dmp/tmp/data/classification/data_handlabeling_cycle1/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2110, 7)\n"
     ]
    }
   ],
   "source": [
    "def get_datasheet(url, sheetname):\n",
    "    # extract specific sheet from google doc\n",
    "    xls = pd.ExcelFile(url)\n",
    "    return pd.read_excel(xls, sheetname)\n",
    "\n",
    "def deduplicate(df, columnlist):\n",
    "    \n",
    "    df.drop_duplicates(columnlist,inplace=True)\n",
    "    return df\n",
    "\n",
    "sheetname = 'error_combined (1).csv'\n",
    "df_handlabeled = get_datasheet(URL0, sheetname)\n",
    "print(df_handlabeled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9855, 7)\n"
     ]
    }
   ],
   "source": [
    "def clean_idx(data):\n",
    "    data = data[data['idx'].notnull()]\n",
    "    data['idx'] = data['idx'].map(int)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def generate_original_df(URL1, URL2):\n",
    "\n",
    "    df1 = pd.read_csv(URL1, sep='|')\n",
    "    df2 = pd.read_csv(URL2, sep='|')\n",
    "\n",
    "    df_original = pd.concat([df1, df2])\n",
    "    \n",
    "    return df_original\n",
    "\n",
    "df_original = generate_original_df(URL1, URL2)\n",
    "print (df_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9670, 7)\n",
      "(9830, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "def merge_original_handlabeled(df_original, df_handlabeled):\n",
    "    \n",
    "    # remove NA\n",
    "    df_handlabeled = df_handlabeled[df_handlabeled.Re_category.notnull()]\n",
    "    \n",
    "    # deduplicate\n",
    "    df_handlabeled = deduplicate(df_handlabeled, ['idx'])\n",
    "    df_original = deduplicate(df_original, ['idx'])\n",
    "    \n",
    "    # df_original_kept(merge later)\n",
    "    df_original_kept = df_original[~df_original['idx'].isin(df_handlabeled['idx'].values)]\n",
    "    print (df_original_kept.shape)\n",
    "\n",
    "    # merge\n",
    "    df_merged = pd.merge(df_original[df_original['idx'].isin(df_handlabeled['idx'].values)], \n",
    "                         df_handlabeled.loc[:,['idx','Re_category']], how ='left', left_on=['idx'], right_on=['idx'])\n",
    "    \n",
    "    # remove baddata\n",
    "    df_merged = df_merged[df_merged.Re_category != 'BADDATA']\n",
    "    \n",
    "    # drop column\n",
    "    df_merged.drop(['category'], axis=1, inplace=True)\n",
    "    \n",
    "    # rename column\n",
    "    df_merged.rename(columns={'Re_category': 'category'}, inplace=True)\n",
    "    \n",
    "    # merge horizontally\n",
    "    df_merged = pd.concat([df_original_kept, df_merged])\n",
    "    \n",
    "    del df_original_kept\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "\n",
    "df_temp = merge_original_handlabeled(df_original, df_handlabeled)\n",
    "print(df_temp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate train, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9830, 7)\n",
      "EN_US_SPORTS           1102\n",
      "EN_US_ENTERTAINMENT     977\n",
      "EN_US_SCIENCE           955\n",
      "EN_US_POLITICS          939\n",
      "EN_US_WORLD             936\n",
      "EN_US_TECHNOLOGY        914\n",
      "EN_US_NATIONAL          761\n",
      "EN_US_LIFESTYLE         715\n",
      "EN_US_BUSINESS          565\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_temp = df_temp[df_temp['category'] != 'UNKNOWN']\n",
    "print(df_temp.shape)\n",
    "\n",
    "train, test = train_test_split(df_temp, test_size=0.2, random_state=42)\n",
    "print (train.category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move files to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', sep='|', index = False)\n",
    "test.to_csv('test.csv', sep='|', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import boto3\n",
    "\n",
    "def upload_to_s3(bucket, folder, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = 'tmp/data/classification/' + folder + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "bucket = 'smart-newsdev-dmp'\n",
    "folder = 'data_handlabeling_cycle2/' ############### double-check\n",
    "\n",
    "upload_to_s3(bucket, folder, 'train.csv')\n",
    "upload_to_s3(bucket, folder, 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Manually add tag (SageMaker = true) on the new files in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
