{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Generate three dataframes -------------------------\n",
    "# df_original --- train1800 and test1800 in data_original\n",
    "# df_error    --- result of out of bag forecast in data_handlabeling_cycle1\n",
    "# df_handlabeled --- hand-labeled data in data_handlabeling_cycle1\n",
    "\n",
    "### Analyze handlabeled data -------------------------\n",
    "## df_merged  --- df_original + df_handlabeled\n",
    "\n",
    "### Refine handlabeled data -------------------------\n",
    "## then generate df_refined, df_conflict, df_one_unknown from df_merged\n",
    "\n",
    "### Generate train, test data -------------------------\n",
    "## then generate train, test from df_refined + df_original_kept\n",
    "\n",
    "### Move files to s3\n",
    "## offline: df_conflict, df_one_unknown, train_refined, test_refined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate three dfs\n",
    "\n",
    "- df_error    --- result of out of bag forecast\n",
    "- df_original_kept --- from (df_original (=train1800 and test1800) minus df_error)\n",
    "- df_handlabeled --- hand-labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Install a pip package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install s3fs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# URL0 = 's3://smart-newsdev-dmp/tmp/data/classification/error_handlabeled_combined.csv'\n",
    "# URL1 = '../data/train1800.csv'\n",
    "# URL2 = '../data/test1800.csv'\n",
    "# URL3 = 'error_combined.csv'\n",
    "\n",
    "URL0 = 's3://smart-newsdev-dmp/tmp/data/classification/data_handlabeling_cycle1/error_handlabeled_combined.csv'\n",
    "URL1 = 's3://smart-newsdev-dmp/tmp/data/classification/data_original/train1800.csv'\n",
    "URL2 = 's3://smart-newsdev-dmp/tmp/data/classification/data_original/test1800.csv'\n",
    "URL3 = 's3://smart-newsdev-dmp/tmp/data/classification/data_handlabeling_cycle1/error_combined.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14465, 6)\n",
      "(6565, 6)\n",
      "(4000, 5)\n",
      "(14465, 6)\n",
      "(8209, 6)\n"
     ]
    }
   ],
   "source": [
    "def clean_idx(data):\n",
    "    data = data[data['idx'].notnull()]\n",
    "    data['idx'] = data['idx'].map(int)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def remove_short_texts(data):\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    data = clean_idx(data)\n",
    "    \n",
    "    data['text'] = data['text'].map(str).apply(lambda x: re.sub(\"\\[article_title\\]\\s\\[article_body\\]\",\"\", x))\n",
    "    data['text'] = data['text'].map(str).apply(lambda x: re.sub(\"News Entertainment Lifestyle Tech & Innovation All Sections\",\"\", x))\n",
    "\n",
    "    data['text'] = data['text'].map(str).apply(lambda x: \" \".join(x.split()))\n",
    "    data['text'] = data['text'].map(str).apply(lambda x: x.strip())\n",
    "    data['title'] = data['title'].map(str).apply(lambda x: x.strip())\n",
    "    \n",
    "    \n",
    "    data['char_length1'] = data['text'].map(str).apply(len)\n",
    "    data = data[data['char_length1'] >= 200]\n",
    "    data['char_length2'] = data['title'].map(str).apply(len)\n",
    "    data = data[data['char_length2'] >= 22]\n",
    "    data.drop(['char_length1', 'char_length2'], axis=1, inplace=True)\n",
    "      \n",
    "    return data\n",
    "\n",
    "def generate_original_df(URL1, URL2):\n",
    "\n",
    "    df1 = pd.read_csv(URL1, sep='|')\n",
    "    df2 = pd.read_csv(URL2, sep='|')\n",
    "\n",
    "    df_original = pd.concat([df1, df2])\n",
    "\n",
    "    df_original = remove_short_texts(df_original)\n",
    "    \n",
    "    return df_original\n",
    "\n",
    "\n",
    "df_original = generate_original_df(URL1, URL2)\n",
    "print (df_original.shape)\n",
    "\n",
    "df_error = pd.read_csv(URL3, sep='|')\n",
    "df_error = clean_idx(df_error)\n",
    "print (df_error.shape)\n",
    "\n",
    "df_handlabeled = pd.read_csv(URL0, sep='|')\n",
    "df_handlabeled.drop(['title', 'text', 'url'], axis=1, inplace=True)\n",
    "print (df_handlabeled.shape)\n",
    "\n",
    "print (df_original.shape)\n",
    "df_original_kept = df_original[~df_original['idx'].isin(df_error['idx'].values)]\n",
    "print (df_original_kept.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze handlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows is 4000\n",
      "Number of rows at least one are labeled is 2829\n",
      "Breakdown ---------------------------------------------------------------\n",
      "2 Unknowns is 8\n",
      "2 baddata is 0\n",
      "2 votes is 438\n",
      "1 Baddata is 392\n",
      "1 vote + 'UNKNOWN' is 201\n",
      "1 vote + non-vote is 1619\n",
      "Conflict votes is 171\n"
     ]
    }
   ],
   "source": [
    "def merge_original_handlabeled(df_original, df_handlabeled):\n",
    "\n",
    "    df_merged = pd.merge(df_original[df_original['idx'].isin(df_handlabeled['idx'].values)], \n",
    "                         df_handlabeled, how ='left', left_on=['idx'], right_on=['idx'])\n",
    "    \n",
    "    return df_merged\n",
    "\n",
    "def assess_df(df):\n",
    "    # fill na\n",
    "    df['Category_person1'].fillna(0, inplace=True)\n",
    "    df['Category_person2'].fillna(0, inplace=True) \n",
    "    \n",
    "    total_rows = df.shape[0]\n",
    "    null_rows = df[(df['Category_person1'] ==0) & (df['Category_person2'] == 0)].shape[0] # both nan\n",
    "    print (\"Number of rows is {}\".format(total_rows))\n",
    "    print (\"Number of rows at least one are labeled is {}\".format(total_rows - null_rows))\n",
    "    \n",
    "    df = df[(df['Category_person1']!=0) | (df['Category_person2']!=0)] # remove 0 and 0\n",
    "    \n",
    "    print (\"Breakdown ---------------------------------------------------------------\")\n",
    "    \n",
    "    print (\"2 Unknowns is {}\".format(df[(df['Category_person1']=='UNKNOWN')& (df['Category_person2']=='UNKNOWN')].shape[0]))\n",
    "    df = df[(df['Category_person1'] !='UNKNOWN') | (df['Category_person2'] !='UNKNOWN')] # remove 2 unknowns\n",
    "\n",
    "    print (\"2 baddata is {}\".format(df[(df['Category_person1']=='BADDATA')& (df['Category_person2']=='BADDATE')].shape[0]))\n",
    "    df = df[(df['Category_person1'] !='BADDATE') | (df['Category_person2'] !='BADDATE')] # remove 2 baddata\n",
    "    \n",
    "    print (\"2 votes is {}\".format(df [df['Category_person1'] == df['Category_person2'] ].shape[0]))\n",
    "    df = df [df['Category_person1'] != df['Category_person2'] ] # remove 2 votes\n",
    "\n",
    "    print (\"1 Baddata is {}\".format(df[(df['Category_person1']=='BADDATA') | (df['Category_person2']=='BADDATA')].shape[0]))\n",
    "    df = df[(df['Category_person1'] !='BADDATA') & (df['Category_person2'] !='BADDATA')] # remove 1 baddata\n",
    "    \n",
    "    print (\"1 vote + 'UNKNOWN' is {}\".format(df [(df['Category_person1']=='UNKNOWN') | (df['Category_person2']=='UNKNOWN') ].shape[0]))\n",
    "    df = df[(df['Category_person1'] != 'UNKNOWN') & (df['Category_person2'] != 'UNKNOWN')] # removce 1 unknown\n",
    "    \n",
    "    print (\"1 vote + non-vote is {}\".format(df [(df['Category_person1']==0) | (df['Category_person2']==0) ].shape[0]))\n",
    "    df = df[(df['Category_person1'] != 0) & (df['Category_person2'] != 0)] # remove 1 label\n",
    "    \n",
    "    print (\"Conflict votes is {}\".format(df.shape[0]))\n",
    "    \n",
    "    \n",
    "df_merged = merge_original_handlabeled(df_original, df_handlabeled)\n",
    "df_merged.shape\n",
    "assess_df(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine handlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_refined:\n",
      "2271\n",
      "category == re_category:\n",
      "1084\n",
      "(2271, 6)\n",
      "(8209, 6)\n",
      "(10480, 6)\n"
     ]
    }
   ],
   "source": [
    "def refine_df(df):\n",
    "    # fillna with 0\n",
    "    df['Category_person1'].fillna(0, inplace=True)\n",
    "    df['Category_person2'].fillna(0, inplace=True)    \n",
    "    \n",
    "    # remove nan and nan\n",
    "    df = df[(df['Category_person1']!=0) | (df['Category_person2']!=0)] # remove 0 and 0\n",
    "    \n",
    "    # remove two unknowns (keep one known)\n",
    "    df = df[(df['Category_person1'] !='UNKNOWN') | (df['Category_person2'] !='UNKNOWN')]\n",
    "    \n",
    "    # remove one baddata\n",
    "    df = df[(df['Category_person1'] !='BADDATA') & (df['Category_person2'] !='BADDATA')]\n",
    "    \n",
    "    \n",
    "    # create new column 're_category'\n",
    "    def column_rule(row):\n",
    "        \n",
    "        # two votes\n",
    "        if row['Category_person1'] == row['Category_person2']:\n",
    "            val = row['Category_person1']            \n",
    "        \n",
    "        # prioritize world\n",
    "        elif row['Category_person1'] == 'EN_US_WORLD' or row['Category_person2'] == 'EN_US_WORLD':\n",
    "            val = 'EN_US_WORLD'\n",
    "            \n",
    "        # overwrite in case either one is null or unknown\n",
    "        elif row['Category_person1'] and (row['Category_person2'] == 0  or row['Category_person2'] == 'UNKNOWN'):\n",
    "            val = row['Category_person1']\n",
    "        \n",
    "        # overwrite in case either one is null or unknown\n",
    "        elif (row['Category_person1'] ==0 or row['Category_person1'] == 'UNKNOWN') and row['Category_person2']:\n",
    "            val = row['Category_person2']\n",
    "        \n",
    "        else:\n",
    "            val = -1\n",
    "            \n",
    "        return val\n",
    "    \n",
    "    df['re_category'] = df.apply(column_rule, axis=1)\n",
    "    \n",
    "    df_conflict = df[df['re_category'] == -1].loc[:,['idx','title','text','url','category','Category_person1','note_person1','Category_person2','note_person2']]\n",
    "    \n",
    "    df_refined = df[df['re_category'] != -1]\n",
    "    \n",
    "    print ('df_refined:')\n",
    "    print (df_refined.shape[0])\n",
    "    print ('category == re_category:')\n",
    "    print (df_refined[df_refined['re_category'] == df_refined['category']].shape[0])\n",
    "    \n",
    "    df_one_unknown = df[(df['Category_person1'] == 'UNKNOWN')| (df['Category_person2'] == 'UNKNOWN')].loc[:,['idx','title','text','url','category','Category_person1','note_person1','Category_person2','note_person2']]\n",
    "    \n",
    "    \n",
    "    return df_refined, df_conflict, df_one_unknown\n",
    "\n",
    "df_refined, df_conflict, df_one_unknown = refine_df(df_merged)\n",
    "\n",
    "df_refined.drop(['Category_person1', 'note_person1','Category_person2','note_person2','category'], axis=1, inplace=True)\n",
    "df_refined.rename(columns={'re_category': 'category'}, inplace=True)\n",
    "\n",
    "print (df_refined.shape)\n",
    "\n",
    "print (df_original_kept.shape)\n",
    "\n",
    "df_temp = pd.concat([df_original_kept, df_refined])\n",
    "\n",
    "print (df_temp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate train, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10374, 7)\n",
      "(9855, 7)\n",
      "EN_US_SPORTS           1117\n",
      "EN_US_ENTERTAINMENT     970\n",
      "EN_US_SCIENCE           966\n",
      "EN_US_POLITICS          926\n",
      "EN_US_WORLD             923\n",
      "EN_US_TECHNOLOGY        914\n",
      "EN_US_NATIONAL          740\n",
      "EN_US_BUSINESS          686\n",
      "EN_US_LIFESTYLE         642\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_temp = df_temp[df_temp['category'] != 'UNKNOWN']\n",
    "\n",
    "# remove shorter text again\n",
    "df_temp['char_length1'] = df_temp['text'].map(str).apply(len)\n",
    "df_temp = df_temp[df_temp['char_length1'] >= 200]\n",
    "print (df_temp.shape)\n",
    "\n",
    "# remove duplicates\n",
    "df_temp.drop_duplicates(['idx'],inplace=True)\n",
    "print (df_temp.shape)\n",
    "\n",
    "train, test = train_test_split(df_temp, test_size=0.2, random_state=42)\n",
    "print (train.category.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move files to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', sep='|', index = False)\n",
    "test.to_csv('test.csv', sep='|', index = False)\n",
    "df_conflict.to_csv('conflict.csv', sep=',', index = False)\n",
    "df_one_unknown.to_csv('one_unknown.csv', sep=',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import boto3\n",
    "\n",
    "def upload_to_s3(bucket, folder, file):\n",
    "    s3 = boto3.resource('s3')\n",
    "    data = open(file, \"rb\")\n",
    "    key = 'tmp/data/classification/' + folder + file\n",
    "    s3.Bucket(bucket).put_object(Key=key, Body=data)\n",
    "\n",
    "bucket = 'smart-newsdev-dmp'\n",
    "folder = 'data_handlabeling_cycle1/'\n",
    "\n",
    "upload_to_s3(bucket, folder, 'conflict.csv')\n",
    "upload_to_s3(bucket, folder, 'one_unknown.csv')\n",
    "upload_to_s3(bucket, folder, 'train.csv')\n",
    "upload_to_s3(bucket, folder, 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Manually add tag (SageMaker = true) on the new files in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
